{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "LoRA is an acronym for Low-Rank Adaptation or Low-Rank Adaptors. It is an efficient and lightweight method for fine-tuning pre-existing language models including BERT, RoBERTa, GPT, Llama, and Mistral. LoRA achieve high efficiency by utilizing fewer paramters, consequently lower computation complexity and memory usage. This enable consumers to train large models on consumer-grade GPUs and effortlessly distribute our compact (in terms of megabytes) LoRAs to others. Let's conceptualize how LoRA works by fine-tuning an LLM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
