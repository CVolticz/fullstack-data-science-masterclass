{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark\n",
    "\n",
    "We've just learned about some of the design patterns in Hadoop MapReduce. However, being a legacy technology and for what it is worth, Hadoop requires a significant amount of overhead to properly setup and configure even with a modern Docker container. Instead, we will look at the limitations of Hadoop MapReduce through the lens of a more modern distributed framework like Spark. By abstracting away many parallelization details Spark provides a flexible interface for the programmer. However a word of warning: don't let the ease of implementation lull you into complacency, scalable solutions still require attention to the details of smart algorithm design. \n",
    "\n",
    "Our goal is to get you up to speed and coding in Spark as quickly as possible; this is by no means a comprehensive tutorial. By the end of today's demo you should be able to:  \r\n",
    "* ... __initialize__ a `SparkSession` in a local NB and use it to run a Spark Job.\r\n",
    "* ... __access__ the Spark Job Tracker UI.\r\n",
    "* ... __describe__ and __create__ RDDs from files or local Python objects.\r\n",
    "* ... __explain__ the difference between actions and transformations.\r\n",
    "* ... __decide__ when to `cache` or `broadcast` part of your data.\r\n",
    "* ... __implement__ Word Counting, Sorting and Naive Bayes in Spark. \r\n",
    "\r\n",
    "__`NOTE:`__ Although RDD successor datatype, Spark dataframes, are becoming more common in production settings we've made a deliberate choice to teach you RDDs first beause building homegrown algorithm implementations is crucial to developing a deep understanding of machine learning and parallelization concepts -- which is the goal of this course. We'll still touch on dataframes in Week 5 when talking about Spark efficiency considerations and we'll do a deep dive into Spark dataframes and streaming solutions in Week 12.\r\n",
    "\r\n",
    "__`Additional Resources:`__ The offical documentation pages offer a user friendly overview of the material covered in this week's readings: [Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make data directory if it doesn't already exist\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\r\n",
    "Today we'll mostly be working with toy examples & data created on the fly in Python. However at the end of this demo we'll revisit Word Count & Naive Bayes using some of the dat3. Run the following cells to re-load the _Alice in Wonderland_ text & the 'Chinese' toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes toy example data paths - ADJUST AS NEEDED\n",
    "TRAIN_PATH = PWD + \"/data/chineseTrain.txt\"\n",
    "TEST_PATH = PWD + \"/data/chineseTest.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Getting started with Spark. \r",
    "For more information, please rread Ch 3-4 from _Learning Spark: Lightning-Fast Big Data Analysis_ by Karau et. al. as well as a few blog posts that set the stage for Spark. From these readings you should be familiar with each of the following terms:\r\n",
    "\r\n",
    "* __Spark session__\r\n",
    "* __Spark context__\r\n",
    "* __driver program__\r\n",
    "* __executor nodes__\r\n",
    "* __resilient distributed datasets (RDDs)__\r\n",
    "* __pair RDDs__\r\n",
    "* __actions__ and __transformations__\r\n",
    "* __lazy evaluation__\r\n",
    "\r\n",
    "The first code block below shows you how to start a `SparkSession` in a Jupyter Notebook. Next we show a simple example of creating and transforming a Spark RDD. Let's use this as a quick vocab review before we dive into more interesting examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"pyspark_demo\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small example\n",
    "myData = sc.parallelize(range(1,100))\n",
    "squares = myData.map(lambda x: (x,x**2))\n",
    "oddSquares = squares.filter(lambda x: x[1] % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (3, 9), (5, 25), (7, 49), (9, 81)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddSquares.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __DISCUSSION QUESTIONS:__ For each key term from the reading, briefly explain what it means in the context of this demo code. Specifically:\n",
    " * _What is the 'driver program' here?_\n",
    " * _What does the spark context do? Do we have 'executors' per se?_\n",
    " * _List all RDDs and pair RDDs present in this example._\n",
    " * _List all transformations present in this example._\n",
    " * _List all actions present in this example._\n",
    " * _What does the concept of 'lazy evaluation' mean about the time it would take to run each cell in the example?_\n",
    " * _If we were working on a cluster, where would each transformation happen? would the data get shuffled?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
