{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark\n",
    "\n",
    "We've just learned about some of the design patterns in Hadoop MapReduce. However, being a legacy technology and for what it is worth, Hadoop requires a significant amount of overhead to properly setup and configure even with a modern Docker container. Instead, we will look at the limitations of Hadoop MapReduce through the lens of a more modern distributed framework like Spark. By abstracting away many parallelization details Spark provides a flexible interface for the programmer. However a word of warning: don't let the ease of implementation lull you into complacency, scalable solutions still require attention to the details of smart algorithm design. \n",
    "\n",
    "Our goal is to get you up to speed and coding in Spark as quickly as possible; this is by no means a comprehensive tutorial. By the end of today's demo you should be able to:  \n",
    "* ... __initialize__ a `SparkSession` in a local NB and use it to run a Spark Job.\n",
    "* ... __access__ the Spark Job Tracker UI.\n",
    "* ... __describe__ and __create__ RDDs from files or local Python objects.\n",
    "* ... __explain__ the difference between actions and transformations.\n",
    "* ... __decide__ when to `cache` or `broadcast` part of your data.\n",
    "* ... __implement__ Word Counting, Sorting and Naive Bayes in Spark. \n",
    "\n",
    "__`NOTE:`__ Although RDD successor datatype, Spark dataframes, are becoming more common in production settings we've made a deliberate choice to teach you RDDs first beause building homegrown algorithm implementations is crucial to developing a deep understanding of machine learning and parallelization concepts -- which is the goal of this course. We'll still touch on dataframes in Week 5 when talking about Spark efficiency considerations and we'll do a deep dive into Spark dataframes and streaming solutions in Week 12.\n",
    "\n",
    "__`Additional Resources:`__ The offical documentation pages offer a user friendly overview of the material covered in this week's readings: [Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make data directory if it doesn't already exist\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Today we'll mostly be working with toy examples & data created on the fly in Python. However at the end of this demo we'll revisit Word Count & Naive Bayes using some of the dat3. Run the following cells to re-load the _Alice in Wonderland_ text & the 'Chinese' toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "# (Re)Download Alice Full text from Project Gutenberg - RUN THIS CELL AS IS (if Option 1 failed)\n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl \"http://www.gutenberg.org/files/11/11-0.txt\" -o data/alice.txt\n",
    "ALICE_TXT = PWD + \"/data/alice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes toy example data paths - ADJUST AS NEEDED\n",
    "TRAIN_PATH = PWD + \"/data/chineseTrain.txt\"\n",
    "TEST_PATH = PWD + \"/data/chineseTest.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. Getting started with Spark. \n",
    "For more information, please read Ch 2 from [High Performance Spark\n",
    "](https://learning.oreilly.com/library/view/high-performance-spark/9781491943199/) by Karau et al. and Ch 3-4 from [Learning Spark: Lightning-Fast Big Data Analysis](https://www.oreilly.com/library/view/learning-spark/9781449359034/) by Karau et. al. as well as a few blog posts that set the stage for Spark. From these readings you should be familiar with each of the following terms:\n",
    "\n",
    "* __Spark session__\n",
    "* __Spark context__\n",
    "* __driver program__\n",
    "* __executor nodes__\n",
    "* __resilient distributed datasets (RDDs)__\n",
    "* __pair RDDs__\n",
    "* __actions__ and __transformations__\n",
    "* __lazy evaluation__\n",
    "\n",
    "The first code block below shows you how to start a `SparkSession` in a Jupyter Notebook. Next we show a simple example of creating and transforming a Spark RDD. Let's use this as a quick vocab review before we dive into more interesting examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "app_name = \"pyspark_demo\"\n",
    "master = \"local[*]\"\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small example\n",
    "myData = sc.parallelize(range(1,100))\n",
    "squares = myData.map(lambda x: (x,x**2))\n",
    "oddSquares = squares.filter(lambda x: x[1] % 2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (3, 9), (5, 25), (7, 49), (9, 81)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddSquares.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > __DISCUSSION QUESTIONS:__ For each key term from the reading, briefly explain what it means in the context of this demo code. Specifically:\n",
    " * _What is the 'driver program' here?_\n",
    " * _What does the spark context do? Do we have 'executors' per se?_\n",
    " * _List all RDDs and pair RDDs present in this example._\n",
    " * _List all transformations present in this example._\n",
    " * _List all actions present in this example._\n",
    " * _What does the concept of 'lazy evaluation' mean about the time it would take to run each cell in the example?_\n",
    " * _If we were working on a cluster, where would each transformation happen? would the data get shuffled?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. RDD transformations warm ups.\n",
    "\n",
    "Here are some more examples of Spark transformations and actions. For each task below, we've provided a few different implementations. Read each example and discuss the differences. Is one implementation better than the other or are the differences cosmetic? You may wish to discuss:\n",
    "* the format of the data after each transformation\n",
    "* memory usage (on executor nodes & in the driver)\n",
    "* time complexity\n",
    "* amount of network transfer\n",
    "* whether or not the data will get shuffled\n",
    "* coding efficiency & readability  \n",
    "\n",
    "Although we're working with tiny demo examples for now, try to imagine how the same code would operate if we were running a large job on a cluster. To aid in your analysis, navigate to the Spark UI (available at http://localhost:4040). To start, you should see a single job -- the job from Exercise 1. Click on the job description to view the DAG for that job. Check back with this UI as you run each version of the tasks below (__Note__: _the stages tab may be particularly helpful when making your comparisons_).\n",
    "\n",
    "#### a) Multiples of 5 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1\n",
    "dataRDD = sc.parallelize(range(1,100))\n",
    "fivesRDD = dataRDD.filter(lambda x: x % 5 == 0)\n",
    "sevensRDD = dataRDD.filter(lambda x: x % 7 == 0)\n",
    "result = fivesRDD.intersection(sevensRDD)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2\n",
    "dataRDD = sc.parallelize(range(1,100))\n",
    "result = dataRDD.filter(lambda x: x % 5 == 0)\\\n",
    "                .filter(lambda x: x % 7 == 0)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 3\n",
    "dataRDD = sc.parallelize(range(1,100))\n",
    "result = dataRDD.filter(lambda x: x % 7 == 0 and x % 5 == 0)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__DISCUSSION QUESTION:__ \n",
    "* What is the task here? Compare/contrast these three implementations.  \n",
    "* Which of these versions require a shuffle? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
