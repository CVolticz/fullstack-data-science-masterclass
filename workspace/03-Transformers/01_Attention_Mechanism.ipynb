{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "As we've learned, the Attention Mechanism is a method that provide textual context to the Artificial Intelligence. Let's first explore how single-headed attention is calculated by hand. Next, we'll cover the implementation of multiheaded attention mechanism in both Pytorch and Tensorflow libraries in Python. Finally, we'll apply our understanding of the attention mechanism to building a quick Weighted Averaging Neural Network (WANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (2.8.4)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.24.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: keras in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim --quiet\n",
    "%pip install tensorflow-datasets --quiet\n",
    "%pip install -U tensorflow-text==2.8.2 --quiet\n",
    "%pip install pydot --quiet\n",
    "%pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import einops\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# natural language processing libraries\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given\n",
    "q = [1, 2., 1]\n",
    "\n",
    "k1 = v1 = [-1, -1, 3.]\n",
    "k2 = v2 = [1, 2, -5.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: 0.5, 0.5\n",
      "Computed Context Vector (Attention Output): [ 0.   0.5 -1. ]\n"
     ]
    }
   ],
   "source": [
    "### Attention Mechanism - Implement the three steps of the attention mechanism\n",
    "\n",
    "# step 1 - attention score using dot product\n",
    "s1 = np.dot(q, k1)\n",
    "s2 = np.dot(q, k2)\n",
    "\n",
    "# step 2 - weights using softmax\n",
    "alpha_1 = np.exp(s1)/sum(np.exp([s1, s2]))\n",
    "alpha_2 = np.exp(s2)/sum(np.exp([s1, s2]))\n",
    "\n",
    "\n",
    "# step 3 - context vector c\n",
    "c = np.add([alpha_1 * i  for i in k1], [alpha_2 * i  for i in k2])\n",
    "\n",
    "print(f\"Attention weights: {alpha_1}, {alpha_2}\")\n",
    "print(f\"Computed Context Vector (Attention Output): {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Implementation In TensorFlow/Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector: [[1. 2. 1.]]\n",
      "Key Vector: \n",
      "[[-1. -1.  3.]\n",
      " [ 1.  2. -5.]]\n",
      "Value Vector: \n",
      "[[-1. -1.  3.]\n",
      " [ 1.  2. -5.]]\n"
     ]
    }
   ],
   "source": [
    "# Trying with Keras API\n",
    "test_query = np.array([q])\n",
    "test_keys_values = np.array([k1, k2])\n",
    "\n",
    "print(f\"Query Vector: {test_query}\")\n",
    "print(f\"Key Vector: \\n{test_keys_values}\")\n",
    "print(f\"Value Vector: \\n{test_keys_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Outputs:  tf.Tensor([[ 0.   0.5 -1. ]], shape=(1, 3), dtype=float32)\n",
      "Attention Weights:  tf.Tensor([[0.5 0.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# convert the arrays to tensors\n",
    "test_query_tf = tf.convert_to_tensor(test_query)\n",
    "test_keys_values_tf = tf.convert_to_tensor(test_keys_values)\n",
    "\n",
    "# Apply the scaled dot-product attention\n",
    "attention_output, attention_weights = tf.keras.layers.Attention()([test_query_tf, test_keys_values_tf], return_attention_scores=True)\n",
    "\n",
    "print(\"Attention Outputs: \", attention_output)\n",
    "print(\"Attention Weights: \", attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Outputs:  tensor([[ 5.9605e-08,  5.0000e-01, -1.0000e+00]])\n",
      "Attention Weights:  tensor([[0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the arrays to PyTorch tensors\n",
    "test_query_ptorch = torch.tensor(test_query, dtype=torch.float32) # (sequence_length, batch_size, embed_dim)\n",
    "test_keys_values_ptorch = torch.tensor(test_keys_values, dtype=torch.float32) # (sequence_length, batch_size, embed_dim)\n",
    "\n",
    "# Apply the scaled dot-product attention\"\n",
    "attention_output = nn.functional.scaled_dot_product_attention(test_query_ptorch, test_keys_values_ptorch, test_keys_values_ptorch, dropout_p=0.0, is_causal=False)\n",
    "\n",
    "# Compute the attention scores (scaled dot-product)\n",
    "d_k = test_query_ptorch.size(-1)  # Embedding dimension\n",
    "scores = torch.matmul(test_query_ptorch, test_keys_values_ptorch.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "# Apply the softmax to get the attention weights\n",
    "attention_weights = nn.functional.softmax(scores, dim=-1) \n",
    "\n",
    "print(\"Attention Outputs: \", attention_output)\n",
    "print(\"Attention Weights: \", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"WANN\" Model\n",
    "For our next part, we'll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
