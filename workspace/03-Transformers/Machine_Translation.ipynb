{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "While it might not seem like it, one of the very first development in Artificial Intellience is Neural Machine Translation. Traditionally, machine translation is a challenging task that involves large statistical models developed using high sophisticated linguistic knowledge. In neural machine translation, deep neural networks are developed for the problem. Advancing toward using Artificial Intelligence in machine translation task, AI to look for patterns in the input language and provide the target language representations as output. \n",
    "\n",
    "## Data Preparation\n",
    "Like many machine learning task, we need to start with the data. In this tutorial, we'll use a dataset of English to Vietnamese phrases. Think of this as learning Vietnamese or English using flashcards. The dataset can be download [here](https://www.kaggle.com/datasets/hungnm/englishvietnamese-translation). To prepare the dataset for modeling, we'll perform the following:\n",
    "\n",
    "1. Start by reading in the associated data and scan through it\n",
    "2. Cleanup punctuation\n",
    "3. Process upper and lowercase words\n",
    "4. Processing special characters\n",
    "5. Handle duplciate phrases in English with different translations in Vietnamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\n",
      "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: Click>=6.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (8.1.7)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: nltk in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (4.66.1)\n",
      "Requirement already satisfied: requests in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (2.31.0)\n",
      "Requirement already satisfied: joblib in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (1.3.2)\n",
      "Collecting scikit-learn (from underthesea)\n",
      "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: PyYAML in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from underthesea) (6.0.1)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from nltk->underthesea) (2023.12.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->underthesea) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->underthesea) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->underthesea) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from requests->underthesea) (2022.12.7)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ktrinh/miniconda3/envs/torch_env/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->underthesea)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: underthesea-core, python-crfsuite, threadpoolctl, scikit-learn, underthesea\n",
      "Successfully installed python-crfsuite-0.9.10 scikit-learn-1.5.0 threadpoolctl-3.5.0 underthesea-6.8.4 underthesea-core-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# NLP libraries\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize  # Vietnamese NLP Toolkit\n",
    "\n",
    "# NN Libraries\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def load_data(file_path:str) -> list:\n",
    "  \"\"\"\n",
    "    Function to load data from a text file\n",
    "    Read it line by line and return as a list of strings\n",
    "    Inputs:\n",
    "      - file_path {string}: path to the file to be read\n",
    "    Outputs:\n",
    "      - data {list}: list of strings\n",
    "  \"\"\"\n",
    "\n",
    "  data = []\n",
    "  with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "    # read file line by line\n",
    "    for line in file:\n",
    "      # remove leading and trailing whitespaces\n",
    "      line = line.strip()\n",
    "      # append to data list\n",
    "      data.append(line)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def to_pairs(doc1: list, doc2: list) -> list:\n",
    "  \"\"\"\n",
    "    Function to convert join two lists of strings into a list of pairs\n",
    "    Inputs:\n",
    "      - doc1 {list}: list of strings\n",
    "      - doc2 {list}: list of strings\n",
    "    Outputs:\n",
    "      - pairs {list}: list of pairs\n",
    "  \"\"\"\n",
    "  # initialize list of pairs\n",
    "  pairs = []\n",
    "  for i in range(0, len(doc1)):\n",
    "\n",
    "    # append pair of strings\n",
    "    pairs.append([doc1[i], doc2[i]])\n",
    "\n",
    "  return pairs\n",
    "\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines: list) -> np.array:\n",
    "  \"\"\"\n",
    "    Function to clean a list of pairs of strings\n",
    "    Inputs:\n",
    "      - lines {list}: list of pairs of strings\n",
    "    Outputs:\n",
    "      - cleaned {list}: list of cleaned pairs of\n",
    "  \"\"\"\n",
    "\n",
    "  # delcare list and prepare regex for char filtering\n",
    "  # also prepare translation table for removing punctuation\n",
    "  cleaned = list()\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "  for pair in tqdm(lines):\n",
    "    clean_pair = list()\n",
    "    # for each pari, perform the following operations\n",
    "    # 1. tokenize on white space\n",
    "    # 2. convert to lowercase\n",
    "    # 3. remove punctuation from each token \n",
    "    # 4. remove extra whitespaces\n",
    "    # 5. remove tokens with numbers in them\n",
    "    # 6. store as string\n",
    "    for line in pair:\n",
    "      line = line.split()\n",
    "      line = [word.lower() for word in line]\n",
    "      line = [word.translate(table) for word in line]\n",
    "      line = [re.sub(\"\\s+\", \" \", w) for w in line]\n",
    "      line = [word for word in line if word.isalpha()]\n",
    "      clean_pair.append(' '.join(line))\n",
    "      cleaned.append(clean_pair)\n",
    "  return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please put the dustpan in the broom closet', 'Be quiet for a moment.', 'Read this', 'Tom persuaded the store manager to give him back his money.', 'Friendship consists of mutual understanding']\n",
      "['xin vui lòng đặt đồ hốt rác trong tủ chổi', 'im lặng một lát', 'đọc này', 'tom thuyết phục người quản lý cửa hàng trả lại tiền cho anh ta.', 'tình bạn bao gồm sự hiểu biết lẫn nhau']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 254090, 254090)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "# From initial inspection, the data between the English and Vietnamese sentences are aligned\n",
    "# So we can read them in as pairs\n",
    "english_text = load_data(DATA_DIR + 'raw/en_sents.txt')\n",
    "vietnamese_text = load_data(DATA_DIR + 'raw/vi_sents.txt')\n",
    "print(english_text[:5]), print(vietnamese_text[:5]), len(english_text), len(vietnamese_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Please put the dustpan in the broom closet',\n",
       "  'xin vui lòng đặt đồ hốt rác trong tủ chổi'],\n",
       " ['Be quiet for a moment.', 'im lặng một lát'],\n",
       " ['Read this', 'đọc này'],\n",
       " ['Tom persuaded the store manager to give him back his money.',\n",
       "  'tom thuyết phục người quản lý cửa hàng trả lại tiền cho anh ta.'],\n",
       " ['Friendship consists of mutual understanding',\n",
       "  'tình bạn bao gồm sự hiểu biết lẫn nhau']]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pairs\n",
    "sentence_pairs = to_pairs(english_text, vietnamese_text)\n",
    "sentence_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecad21388c504a2ebf660aa07f9729ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([['please put the dustpan in the broom closet',\n",
       "        'xin vui lòng đặt đồ hốt rác trong tủ chổi'],\n",
       "       ['please put the dustpan in the broom closet',\n",
       "        'xin vui lòng đặt đồ hốt rác trong tủ chổi'],\n",
       "       ['be quiet for a moment', 'im lặng một lát'],\n",
       "       ['be quiet for a moment', 'im lặng một lát'],\n",
       "       ['read this', 'đọc này']], dtype='<U265')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessed data pairs\n",
    "cleaned_pairs = clean_pairs(sentence_pairs)\n",
    "cleaned_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Now that we have prepared the data, it is time to tokenize it. Tokenization is the process of breaking down a sentence into indivial word, called token, and then assign a numerical value to it. A vocabulary is also created in this process to keep tract of the word to number consersion as well as the total number of unique words in our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source and target lanauge\n",
    "SRC_LANG = 'en'\n",
    "TGT_LANG = 'vi'\n",
    "\n",
    "# Declare word to number and number to word dictionary\n",
    "token_index = {}\n",
    "index_token = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
