{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "While it might not seem like it, one of the very first development in Artificial Intellience is Neural Machine Translation. Traditionally, machine translation is a challenging task that involves large statistical models developed using high sophisticated linguistic knowledge. In neural machine translation, deep neural networks are developed for the problem. Advancing toward using Artificial Intelligence in machine translation task, AI to look for patterns in the input language and provide the target language representations as output. \n",
    "\n",
    "## Data Preparation\n",
    "Like many machine learning task, we need to start with the data. In this tutorial, we'll use a dataset of English to Vietnamese phrases. Think of this as learning Vietnamese or English using flashcards. The dataset can be download [here](https://www.kaggle.com/datasets/hungnm/englishvietnamese-translation) and the credit for data preprocessing can be found [here](https://www.kaggle.com/code/huhuyngun/english-to-vietnamese-with-transformer) To prepare the dataset for modeling, we'll perform the following:\n",
    "\n",
    "1. Start by reading in the associated data and scan through it\n",
    "2. Cleanup punctuation\n",
    "3. Process upper and lowercase words\n",
    "4. Processing special characters\n",
    "5. Handle duplciate phrases in English with different translations in Vietnamese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install underthesea --quiet\n",
    "!pip install torchtext --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python libraries\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Iterable, List\n",
    "from unicodedata import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# NLP libraries\n",
    "from gensim.models import KeyedVectors\n",
    "from underthesea import word_tokenize  # Vietnamese NLP Toolkit\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "# NN Libraries\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper Functions\n",
    "def load_data(file_path:str) -> List[str]:\n",
    "  \"\"\"\n",
    "    Function to load data from a text file\n",
    "    Read it line by line and return as a list of strings\n",
    "    Inputs:\n",
    "      - file_path {string}: path to the file to be read\n",
    "    Outputs:\n",
    "      - data {list}: list of strings\n",
    "  \"\"\"\n",
    "\n",
    "  data = []\n",
    "  with open(file_path, 'rt', encoding='utf-8') as file:\n",
    "    # read file line by line\n",
    "    for line in file:\n",
    "      # remove leading and trailing whitespaces\n",
    "      line = line.strip()\n",
    "      # append to data list\n",
    "      data.append(line)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def to_pairs(doc1: List[str], doc2: List[str]) -> List[str]:\n",
    "  \"\"\"\n",
    "    Function to convert join two lists of strings into a list of pairs\n",
    "    Inputs:\n",
    "      - doc1 {list}: list of strings\n",
    "      - doc2 {list}: list of strings\n",
    "    Outputs:\n",
    "      - pairs {list}: list of pairs\n",
    "  \"\"\"\n",
    "  # initialize list of pairs\n",
    "  pairs = []\n",
    "  for i in range(0, len(doc1)):\n",
    "\n",
    "    # append pair of strings\n",
    "    pairs.append([doc1[i], doc2[i]])\n",
    "\n",
    "  return pairs\n",
    "\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines: List[str]) -> np.array:\n",
    "  \"\"\"\n",
    "    Function to clean a list of pairs of strings\n",
    "    Inputs:\n",
    "      - lines {list}: list of pairs of strings\n",
    "    Outputs:\n",
    "      - cleaned {list}: list of cleaned pairs of\n",
    "  \"\"\"\n",
    "\n",
    "  # delcare list and prepare regex for char filtering\n",
    "  # also prepare translation table for removing punctuation\n",
    "  cleaned = list()\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "  for pair in tqdm(lines):\n",
    "    clean_pair = list()\n",
    "    # for each pari, perform the following operations\n",
    "    # 1. tokenize on white space\n",
    "    # 2. convert to lowercase\n",
    "    # 3. remove punctuation from each token \n",
    "    # 4. remove extra whitespaces\n",
    "    # 5. remove tokens with numbers in them\n",
    "    # 6. store as string\n",
    "    for line in pair:\n",
    "      line = line.split()\n",
    "      line = [word.lower() for word in line]\n",
    "      line = [word.translate(table) for word in line]\n",
    "      line = [re.sub(\"\\s+\", \" \", w) for w in line]\n",
    "      line = [word for word in line if word.isalpha()]\n",
    "      clean_pair.append(' '.join(line))\n",
    "      cleaned.append(clean_pair)\n",
    "  return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please put the dustpan in the broom closet', 'Be quiet for a moment.', 'Read this', 'Tom persuaded the store manager to give him back his money.', 'Friendship consists of mutual understanding']\n",
      "['xin vui lòng đặt đồ hốt rác trong tủ chổi', 'im lặng một lát', 'đọc này', 'tom thuyết phục người quản lý cửa hàng trả lại tiền cho anh ta.', 'tình bạn bao gồm sự hiểu biết lẫn nhau']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, 254090, 254090)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "# From initial inspection, the data between the English and Vietnamese sentences are aligned\n",
    "# So we can read them in as pairs\n",
    "english_text = load_data(DATA_DIR + 'raw/en_sents.txt')\n",
    "vietnamese_text = load_data(DATA_DIR + 'raw/vi_sents.txt')\n",
    "print(english_text[:5]), print(vietnamese_text[:5]), len(english_text), len(vietnamese_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Please put the dustpan in the broom closet',\n",
       "  'xin vui lòng đặt đồ hốt rác trong tủ chổi'],\n",
       " ['Be quiet for a moment.', 'im lặng một lát'],\n",
       " ['Read this', 'đọc này'],\n",
       " ['Tom persuaded the store manager to give him back his money.',\n",
       "  'tom thuyết phục người quản lý cửa hàng trả lại tiền cho anh ta.'],\n",
       " ['Friendship consists of mutual understanding',\n",
       "  'tình bạn bao gồm sự hiểu biết lẫn nhau']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pairs\n",
    "sentence_pairs = to_pairs(english_text, vietnamese_text)\n",
    "sentence_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4803ef8bca498e97c326466f2fd8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/254090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['please put the dustpan in the broom closet'\n",
      "  'xin vui lòng đặt đồ hốt rác trong tủ chổi']\n",
      " ['please put the dustpan in the broom closet'\n",
      "  'xin vui lòng đặt đồ hốt rác trong tủ chổi']\n",
      " ['be quiet for a moment' 'im lặng một lát']\n",
      " ['be quiet for a moment' 'im lặng một lát']\n",
      " ['read this' 'đọc này']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please put the dustpan in the broom closet</td>\n",
       "      <td>xin vui lòng đặt đồ hốt rác trong tủ chổi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please put the dustpan in the broom closet</td>\n",
       "      <td>xin vui lòng đặt đồ hốt rác trong tủ chổi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>be quiet for a moment</td>\n",
       "      <td>im lặng một lát</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be quiet for a moment</td>\n",
       "      <td>im lặng một lát</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>read this</td>\n",
       "      <td>đọc này</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           en  \\\n",
       "0  please put the dustpan in the broom closet   \n",
       "1  please put the dustpan in the broom closet   \n",
       "2                       be quiet for a moment   \n",
       "3                       be quiet for a moment   \n",
       "4                                   read this   \n",
       "\n",
       "                                          vi  \n",
       "0  xin vui lòng đặt đồ hốt rác trong tủ chổi  \n",
       "1  xin vui lòng đặt đồ hốt rác trong tủ chổi  \n",
       "2                            im lặng một lát  \n",
       "3                            im lặng một lát  \n",
       "4                                    đọc này  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessed data pairs\n",
    "cleaned_pairs = clean_pairs(sentence_pairs)\n",
    "print(cleaned_pairs[:5])\n",
    "\n",
    "# Create dataframe with the token pairs\n",
    "df = pd.DataFrame(cleaned_pairs, columns=['en', 'vi'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Now that we have prepared the data, it is time to tokenize it. Tokenization is the process of breaking down a sentence into indivial word, called token, and then assign a numerical value to it. A vocabulary is also created in this process to keep tract of the word to number consersion as well as the total number of unique words in our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0f236b718f4752813e96d80fc3c6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d45d9b52a14246a3eaeaae33820c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create source and target lanauge\n",
    "SRC_LANG = 'en'\n",
    "TGT_LANG = 'vi'\n",
    "\n",
    "# Create word to number and number to word dictionary\n",
    "# token_index is a dictionary that maps a token to its index number\n",
    "# index_token is a dictionary that maps an index number to its token\n",
    "token_index = {}\n",
    "index_token = {}\n",
    "\n",
    "# Declare special tokens and their index\n",
    "# these special tokens are unknown, pad, bos, eos\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "# Tokenizer for vietnamese sentence\n",
    "def vi_tokenizer(text):\n",
    "  return word_tokenize(text, format='text')\n",
    "\n",
    "# instantiating the tokenizer object\n",
    "token_index[SRC_LANG] = get_tokenizer('basic_english')\n",
    "token_index[TGT_LANG] = get_tokenizer(vi_tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:    \n",
    "  for index, data_sample in tqdm(data_iter):\n",
    "      yield token_index[language](data_sample[language])\n",
    "\n",
    "\n",
    "# begin tokenization process\n",
    "for ln in [SRC_LANG, TGT_LANG]:\n",
    "  # Training data Iterator\n",
    "  train_iter = df.iterrows()\n",
    "  # Create torchtext's Vocab object\n",
    "  index_token[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                  min_freq=1,\n",
    "                                                  specials=special_symbols,\n",
    "                                                  special_first=True)\n",
    "  \n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANG, TGT_LANG]:\n",
    "  index_token[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', '<bos>', '<eos>', 'the', 'to', 'i', 'tom', 'you', 'a'],\n",
       " ['<unk>', '<pad>', '<bos>', '<eos>', ' ', 'n', 'h', 't', 'i', 'c'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing our tokenized vocab\n",
    "index_token['en'].get_itos()[:10], index_token['vi'].get_itos()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
