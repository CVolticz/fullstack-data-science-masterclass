{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "Before we can dive into the fun of data science, we need to lay some statistical groundwork. Some of these could be a review if you've ever taken a statistics course previously. Regardless, it'll be worth to create a good foundation that we can continously build upon. \n",
    "\n",
    "In statistics, there are two school of thoughts, Frequentist and Bayesian. Due to the nature of Bayesian statistics, where historically only being able to address a few cases when a priors (a probability the represents what is originally belived before new evidence is introduced) were known, it has been neglected over the years. For that reason, we will start with Frequentist statisitcs and build into Bayesian theory.\n",
    "\n",
    "## Basic Notation\n",
    "Let's start our discussion by aligning ourselves with some basic notation in a way that make it easy for us to talk about probability and statistics.\n",
    "\n",
    "x is a random variable, a scalar quantity, that measured N times.\n",
    "\n",
    "$ x_i $ is a single measurement with i = 1, ..., N\n",
    "\n",
    "{ $ x_i $ } refers to the set of all N measurements\n",
    "\n",
    "We are generally trying to estimate $ h(x) $, the true distribution from which the values of $ x $ are drawn. We will refer to $ h(x) $ as the probability density (distribution) function or the \"pdf\" and is the propobability of a value lying between $ x $ and $ x + dx $. A histogram is an example of a pdf.\n",
    "\n",
    "While $ h(x) $ is the \"true\" distribution (or population pdf), what we measure from the data is the empirical distribution, which is denoted $ f(x) $. So, $ f(x) $ is a model of $ h(x) $. From a frequentist perspective, given infinite data $ f(x) \\rightarrow h(x) $, but in reality measurement errors keep this from being strictly true.\n",
    "\n",
    "If we are attempting to guess a model for $ h(x) $, then the process is parametric. With a model solution we can generate new (simulated) data that should mimic what we measure.\n",
    "\n",
    "If we are not attempting to guess a model, then the process is nonparametic. That is we are just trying to describe the data that we see in the most compact manner that we can, but we are not trying to produce mock data. The histograms that we made last time are an example of a nonparametric method of describing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import general libraries\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "\n",
    "# import sklearn modules\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# initialize seaborn to enhance matplotlib plots\n",
    "sns.set()\n",
    "\n",
    "\n",
    "# Set global font size\n",
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "# this is the same data used in the Bayesian Blocks figure\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 2000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "hx = lambda x: sum([f * sc.stats.cauchy(mu, gamma).pdf(x)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([sc.stats.cauchy(mu, gamma).rvs(int(f * N), random_state=random_state)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# create an evenly spaced number over a specified ranges\n",
    "# in thsi case 1000 number evenly spaced between -10 and 30\n",
    "xgrid = np.linspace(-10, 30, 1000)\n",
    "\n",
    "# Compute density with KDE\n",
    "kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "kde.fit(x[:, None])\n",
    "dens_kde = np.exp(kde.score_samples(xgrid[:, None]))\n",
    "\n",
    "# Compute density via Gaussian Mixtures using a pre-defined number of clusters (13)\n",
    "gmm = GaussianMixture(n_components=13).fit(x.reshape(-1, 1))\n",
    "logprob = gmm.score_samples(xgrid.reshape(-1, 1))\n",
    "fx = lambda j : np.exp(gmm.score_samples(j.reshape(-1, 1)))\n",
    "\n",
    "# plot the results\n",
    "ax.plot(xgrid, \n",
    "        hx(xgrid), \n",
    "        ':', \n",
    "        color='black', \n",
    "        zorder=3,\n",
    "        label=\"$h(x)$, Generating Distribution\")\n",
    "\n",
    "ax.plot(xgrid, \n",
    "        fx(np.array(xgrid)), \n",
    "        '-',\n",
    "        color='gray',\n",
    "        label=\"$f(x)$, parametric (13 Gaussians)\")\n",
    "\n",
    "ax.plot(xgrid, \n",
    "        dens_kde, \n",
    "        '-', \n",
    "        color='black', \n",
    "        zorder=3,\n",
    "        label=\"$f(x)$, non-parametric (KDE)\")\n",
    "\n",
    "# label the plot\n",
    "ax.text(0.02, \n",
    "        0.95, \n",
    "        s=\"%i points\" % N, \n",
    "        ha='left', \n",
    "        va='top',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel('$p(x)$')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a parametric model, you can use the model function to determine $ f(x) $ directly. If you have a nonparametric description of the data, you would have to interpolate to get $ f(x) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(hx(7.132))             # h(x), the true distribution\n",
    "print(fx(np.array([7.132]))) # f(x) for a parametric model\n",
    "\n",
    "# f(x) for a non-parametric model\n",
    "# Can't do this without interpolating between bins since it is not continous\n",
    "print(xgrid[(xgrid>7.05) & (xgrid<7.2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objective\n",
    "Statistics govern the works of a Data Scientist. As such we'll define the learning goal as follow:\n",
    "\n",
    "1. Determine $ f(x) $ from some real (possibly multi-dimensional) data set,\n",
    "2. Find a compact way to describe $ f(x) $ and its uncertainty,\n",
    "3. Compare it to models of $ h(x) $, and then\n",
    "4. Use the knowledge that we have gained in order to interpret new measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
